{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pymongo import MongoClient\n\nclass MongoDBWriter:\n    def __init__(self, uri):\n        self.uri = uri\n        self.client = MongoClient(self.uri)\n\n    def ping_mongodb(self):\n        try:\n            self.client.admin.command('ping')\n            return \"Connected to MongoDB cluster.\"\n        except Exception as e:\n            return f\"Failed to connect to MongoDB cluster. Error: {str(e)}\"\n\n    def read_file_from_dbfs(self, local_filepath):\n        dataframe = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"false\").load(local_filepath)\n        return dataframe\n\n    def write_dataframe(self, dataframe, database, collection):\n        db = self.client[database]\n        col = db[collection]\n        try:\n            dataframe.write.format(\"mongo\")\\\n            .mode(\"overwrite\")\\\n            .option(\"uri\", self.uri)\\\n            .option(\"database\", database)\\\n            .option(\"collection\", collection)\\\n            .save()\n            return \"DataFrame successfully written to MongoDB.\"\n        except Exception as e:\n            return f\"Failed to write DataFrame to MongoDB. Error: {str(e)}\"\n\nfrom pyspark.sql.functions import col\ndef define_data_types(df, list_of_columns, list_of_data_types):\n    for column, data_type in zip(list_of_columns, list_of_data_types):\n        df = df.withColumn(column, col(column).cast(data_type))\n    return df\n#I do know that setting a datatype as string is redundant\nlist_of_columns = ['DATE', 'PATIENT', 'ENCOUNTER', 'CODE', 'DESCRIPTION', 'VALUE', 'UNITS', 'TYPE']\nlist_of_data_types = [\"date\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\"]\n\nuri = \"mongodb+srv://jacobtmatos:Xn9biJaVacXGRN7W@cluster0.un4d3ar.mongodb.net/?retryWrites=true&w=majority\"\nlocal_filepath = \"dbfs:/FileStore/tables/observations.csv\"\ndatabase = \"DB\"\ncollection = \"observations\"\n\nmongo_writer = MongoDBWriter(uri)\nprint(mongo_writer.ping_mongodb())\ndf_observations = mongo_writer.read_file_from_dbfs(local_filepath)\ndf_observations = define_data_types(df_observations, list_of_columns, list_of_data_types)\nprint(mongo_writer.write_dataframe(df_observations, database, collection))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4feff7e7-b220-4148-89d5-6241eb3c70c7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Connected to MongoDB cluster.\nDataFrame successfully written to MongoDB.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["# German, apenas introduzi os ficheiros com que ia trabalhar, patients e conditions (e tambem o observations)\n## Tambem sei que apesar de nao ser especificado podia ter carregado o observations para o DBFS e fazer a leitura a partir dai, talvez fosse mais rapido"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec0bce01-d73f-4090-b244-75a826dc287d","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"jacob_matos_etl","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
